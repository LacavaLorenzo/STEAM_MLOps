{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Empezamos el proceso de ETL importando las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import io\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import json\n",
    "import re\n",
    "import pyarrow.parquet as pq  # Importamos Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamamos al primer dataset, que se encuentra en formato GNU Zip, un formato excelente para comprimir grandes cantidades de datos. Por lo tanto, es preferible usarlos sin descomprimirlos. Para ello usamos la librería GZIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120445, 13)\n",
      "  publisher genres app_name title   url release_date  tags reviews_url specs  \\\n",
      "0      <NA>   <NA>     <NA>  <NA>  <NA>         <NA>  <NA>        <NA>  <NA>   \n",
      "1      <NA>   <NA>     <NA>  <NA>  <NA>         <NA>  <NA>        <NA>  <NA>   \n",
      "2      <NA>   <NA>     <NA>  <NA>  <NA>         <NA>  <NA>        <NA>  <NA>   \n",
      "3      <NA>   <NA>     <NA>  <NA>  <NA>         <NA>  <NA>        <NA>  <NA>   \n",
      "4      <NA>   <NA>     <NA>  <NA>  <NA>         <NA>  <NA>        <NA>  <NA>   \n",
      "\n",
      "  price  early_access  id developer  \n",
      "0  <NA>           NaN NaN      <NA>  \n",
      "1  <NA>           NaN NaN      <NA>  \n",
      "2  <NA>           NaN NaN      <NA>  \n",
      "3  <NA>           NaN NaN      <NA>  \n",
      "4  <NA>           NaN NaN      <NA>  \n"
     ]
    }
   ],
   "source": [
    "df = dd.read_json('Datasets\\steam_games.json.gz', compression='gzip') #Usamos la librería DASK para llamar al json sin descomprimirlo\n",
    "pdf = df.compute() #Utilizamos la función compute de DASK para pasar todo a un DATAFRAME de PANDAS\n",
    "print(pdf.shape) #Imprimimos la cantidad de Filas y Columnas de nuestro DATAFRAME\n",
    "print(pdf.head()) #Imprimimos las primeros 5 lineas de nuestro DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, los datos proporcionados están llenos de nulos, por lo cual debemos empezar la limpieza.\n",
    "Para ello, utilizaremos la función InfoDF, que nos proporcionará información de valor sobre el DATAFRAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general del DataFrame:\n",
      "Cantidad de filas: 120445\n",
      "Cantidad de columnas: 13\n",
      "Cantidad de filas con datos nulos: 97915\n",
      "\n",
      "Porcentaje de datos nulos por fila:\n",
      "0         100.00\n",
      "1         100.00\n",
      "2         100.00\n",
      "3         100.00\n",
      "4         100.00\n",
      "           ...  \n",
      "120440      0.00\n",
      "120441      0.00\n",
      "120442      0.00\n",
      "120443      0.00\n",
      "120444     38.46\n",
      "Length: 120445, dtype: float64\n",
      "\n",
      "Porcentaje de datos nulos por columna:\n",
      "publisher       80.00\n",
      "genres          76.05\n",
      "app_name        73.32\n",
      "title           75.02\n",
      "url             73.32\n",
      "release_date    75.04\n",
      "tags            73.46\n",
      "reviews_url     73.32\n",
      "specs           73.88\n",
      "price           74.46\n",
      "early_access    73.32\n",
      "id              73.32\n",
      "developer       76.06\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def infoDF(pdf):\n",
    "    # Cantidad de filas y columnas\n",
    "    cantidad_filas = pdf.shape[0]\n",
    "    cantidad_columnas = pdf.shape[1]\n",
    "    \n",
    "    # Cantidad de filas con datos nulos\n",
    "    filas_con_nulos = pdf.isnull().any(axis=1).sum()\n",
    "    \n",
    "    # Porcentaje de nulos por fila y por columna\n",
    "    porcentaje_nulos_filas = (pdf.isnull().sum(axis=1) / cantidad_columnas * 100).round(2)\n",
    "    porcentaje_nulos_columnas = (pdf.isnull().sum() / cantidad_filas * 100).round(2)\n",
    "    \n",
    "    # Información general\n",
    "    print(\"Información general del DataFrame:\")\n",
    "    print(f\"Cantidad de filas: {cantidad_filas}\")\n",
    "    print(f\"Cantidad de columnas: {cantidad_columnas}\")\n",
    "    print(f\"Cantidad de filas con datos nulos: {filas_con_nulos}\")\n",
    "    \n",
    "    # Porcentaje de nulos por fila\n",
    "    print(\"\\nPorcentaje de datos nulos por fila:\")\n",
    "    print(porcentaje_nulos_filas)\n",
    "    \n",
    "    # Porcentaje de nulos por columna\n",
    "    print(\"\\nPorcentaje de datos nulos por columna:\")\n",
    "    print(porcentaje_nulos_columnas)\n",
    "\n",
    "infoDF(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, descartaremos todas las filas dentro del DATAFRAME, que contengan puros datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general del DataFrame:\n",
      "Cantidad de filas: 32135\n",
      "Cantidad de columnas: 13\n",
      "Cantidad de filas con datos nulos: 9605\n",
      "\n",
      "Porcentaje de datos nulos por fila:\n",
      "88310      0.00\n",
      "88311      0.00\n",
      "88312      0.00\n",
      "88313      0.00\n",
      "88314     38.46\n",
      "          ...  \n",
      "120440     0.00\n",
      "120441     0.00\n",
      "120442     0.00\n",
      "120443     0.00\n",
      "120444    38.46\n",
      "Length: 32135, dtype: float64\n",
      "\n",
      "Porcentaje de datos nulos por columna:\n",
      "publisher       25.06\n",
      "genres          10.22\n",
      "app_name         0.01\n",
      "title            6.38\n",
      "url              0.00\n",
      "release_date     6.43\n",
      "tags             0.51\n",
      "reviews_url      0.01\n",
      "specs            2.08\n",
      "price            4.29\n",
      "early_access     0.00\n",
      "id               0.01\n",
      "developer       10.27\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pdf = pdf.dropna(how='all').copy() #Borramos las filas con nulos y las reasignamos al dataframe\n",
    "infoDF(pdf) #Volvemos a utilizar la función infoDF para volver a revisar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un mejor análisis, creamos un nuevo DATAFRAME que contenga únicamente filas donde exista al menos UN (1) dato nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataframe_nulos(pdf):\n",
    "    # Identificar filas y columnas con datos nulos\n",
    "    filas_con_nulos = pdf[pdf.isnull().any(axis=1)]\n",
    "    columnas_con_nulos = pdf.loc[:, pdf.isnull().any()]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las filas y columnas que tienen datos nulos\n",
    "    df_nulos = pd.concat([filas_con_nulos, columnas_con_nulos], axis=1)\n",
    "    \n",
    "    return df_nulos\n",
    "df_nulos = crear_dataframe_nulos(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las filas que contenían nulos, vamos a revisar que no queden filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron filas duplicadas en el DataFrame.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def verificar_filas_duplicadas(pdf):\n",
    "    # Verificar si hay filas duplicadas\n",
    "    hay_duplicados = pdf.duplicated().any()\n",
    "    \n",
    "    if hay_duplicados:\n",
    "        print(\"Se encontraron filas duplicadas en el DataFrame.\")\n",
    "    else:\n",
    "        print(\"No se encontraron filas duplicadas en el DataFrame.\")\n",
    "\n",
    "print(verificar_filas_duplicadas(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que por ahora viene todo en orden, para continuar con la limpieza de los datos, debemos corroborar en qué tipo se encuentra cada columna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publisher       string[pyarrow]\n",
       "genres          string[pyarrow]\n",
       "app_name        string[pyarrow]\n",
       "title           string[pyarrow]\n",
       "url             string[pyarrow]\n",
       "release_date    string[pyarrow]\n",
       "tags            string[pyarrow]\n",
       "reviews_url     string[pyarrow]\n",
       "specs           string[pyarrow]\n",
       "price           string[pyarrow]\n",
       "early_access            float64\n",
       "id                      float64\n",
       "developer       string[pyarrow]\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la primera función de este proyecto, necesitamos 3 columnas cruciales: Price, Developer, Release_Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, acomodaremos la columna price, para eliminar todos sus datos nulos y corregir sus valores a tipo float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\1506138911.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4.99 0.   0.   ... 1.99 4.99 4.99]' has dtype incompatible with string, please explicitly cast to a compatible dtype first.\n",
      "  pdf.loc[:, 'price'] = pdf['price'].apply(lambda x: float(x) if pd.to_numeric(x, errors='coerce') == pd.to_numeric(x, errors='coerce') else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "publisher       string[pyarrow]\n",
       "genres          string[pyarrow]\n",
       "app_name        string[pyarrow]\n",
       "title           string[pyarrow]\n",
       "url             string[pyarrow]\n",
       "release_date    string[pyarrow]\n",
       "tags            string[pyarrow]\n",
       "reviews_url     string[pyarrow]\n",
       "specs           string[pyarrow]\n",
       "price                   float64\n",
       "early_access            float64\n",
       "id                      float64\n",
       "developer       string[pyarrow]\n",
       "dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pdf.dropna(subset=['price'])\n",
    "pdf.loc[:, 'price'] = pdf['price'].apply(lambda x: float(x) if pd.to_numeric(x, errors='coerce') == pd.to_numeric(x, errors='coerce') else 0)\n",
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez solucionada la columna price, pasamos a solucionar la columna release_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo la misma lógica, eliminamos las filas donde release_date tenga datos nulos, y analizamos si todas las fechas están en formato YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores incorrectos en la columna 'release_date':\n",
      "88560        Jun 2009\n",
      "88816        Oct 2010\n",
      "88819        Oct 2010\n",
      "88820        Oct 2010\n",
      "88909        Feb 2011\n",
      "90917        Sep 2014\n",
      "91814        Apr 2015\n",
      "94314        Apr 2016\n",
      "94954        Jul 2016\n",
      "98873        Jul 2017\n",
      "101344           SOON\n",
      "103983           2018\n",
      "104953       Jul 2017\n",
      "106530       Apr 2017\n",
      "107790       Jan 2017\n",
      "108995       Nov 2016\n",
      "109065       Nov 2016\n",
      "109456       Oct 2016\n",
      "110678       Jul 2016\n",
      "111237       Jun 2016\n",
      "114031       Aug 2015\n",
      "114785       Jun 2015\n",
      "114960       May 2015\n",
      "115590       Feb 2015\n",
      "115966       Jan 2015\n",
      "116428       Nov 2014\n",
      "116786       Aug 2014\n",
      "117096       Jul 2014\n",
      "117324       May 2014\n",
      "118632       Feb 2013\n",
      "118781       Dec 2012\n",
      "119742       Jul 2010\n",
      "119784       Mar 2010\n",
      "119826       Jan 2010\n",
      "119879       Oct 2009\n",
      "119928       Sep 2009\n",
      "119980       Jun 2009\n",
      "120268    coming soon\n",
      "120318          SOON™\n",
      "Name: release_date, dtype: string\n",
      "\n",
      "Mensaje de error:\n",
      "time data \"Jun 2009\" doesn't match format \"%Y-%m-%d\", at position 223. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n"
     ]
    }
   ],
   "source": [
    "pdf = pdf.dropna(subset=['release_date'])\n",
    "# Función para identificar valores que no coinciden con el formato especificado\n",
    "def encontrar_valores_incorrectos(pdf):\n",
    "    try:\n",
    "        pd.to_datetime(pdf['release_date'], format='%Y-%m-%d', errors='raise')\n",
    "    except ValueError as e:\n",
    "        valores_incorrectos = pdf.loc[pd.to_datetime(pdf['release_date'], errors='coerce').isnull(), 'release_date']\n",
    "        return valores_incorrectos, str(e)\n",
    "# Identificar valores que no coinciden con el formato especificado\n",
    "valores_incorrectos, error_message = encontrar_valores_incorrectos(pdf)\n",
    "\n",
    "if valores_incorrectos is not None:\n",
    "    print(\"\\nValores incorrectos en la columna 'release_date':\")\n",
    "    print(valores_incorrectos)\n",
    "else:\n",
    "    print(\"\\nNo se encontraron valores incorrectos en la columna 'release_date'.\")\n",
    "\n",
    "if error_message:\n",
    "    print(\"\\nMensaje de error:\")\n",
    "    print(error_message)\n",
    "\n",
    "ver_valores = encontrar_valores_incorrectos(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, hay valores que no corresponden al formato YYYY-MM-DD y los debemos tratar por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer el año\n",
    "def extraer_ano(fecha):\n",
    "    # Intentamos extraer el año del formato 'YYYY-MM-DD'\n",
    "    try:\n",
    "        return int(fecha.split('-')[0])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Intentamos extraer el año del formato 'MMM YYYY'\n",
    "    try:\n",
    "        return int(fecha.split()[-1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Si no podemos extraer el año, devolvemos 0\n",
    "    return 0\n",
    "\n",
    "# Aplicamos la función a la columna 'release_date'\n",
    "pdf['release_date'] = pdf['release_date'].apply(extraer_ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, como podemos ver a continuación, la columna price y la columna release_date se encuentran en formato numérico, para un mejor manejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publisher       string[pyarrow]\n",
       "genres          string[pyarrow]\n",
       "app_name        string[pyarrow]\n",
       "title           string[pyarrow]\n",
       "url             string[pyarrow]\n",
       "release_date              int64\n",
       "tags            string[pyarrow]\n",
       "reviews_url     string[pyarrow]\n",
       "specs           string[pyarrow]\n",
       "price                   float64\n",
       "early_access            float64\n",
       "id                      float64\n",
       "developer       string[pyarrow]\n",
       "dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para continuar el proceso de limpieza, debemos tratar de unificar las columnas genres y tags, ya que, como podemos ver a continuación, las mismas los datos son muy similares y se complementan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88310</th>\n",
       "      <td>['Action', 'Casual', 'Indie', 'Simulation', 'S...</td>\n",
       "      <td>['Strategy', 'Action', 'Indie', 'Casual', 'Sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88311</th>\n",
       "      <td>['Free to Play', 'Indie', 'RPG', 'Strategy']</td>\n",
       "      <td>['Free to Play', 'Strategy', 'Indie', 'RPG', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88312</th>\n",
       "      <td>['Casual', 'Free to Play', 'Indie', 'Simulatio...</td>\n",
       "      <td>['Free to Play', 'Simulation', 'Sports', 'Casu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88313</th>\n",
       "      <td>['Action', 'Adventure', 'Casual']</td>\n",
       "      <td>['Action', 'Adventure', 'Casual']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88315</th>\n",
       "      <td>['Action', 'Adventure', 'Simulation']</td>\n",
       "      <td>['Action', 'Adventure', 'Simulation', 'FPS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120439</th>\n",
       "      <td>['Action', 'Adventure', 'Casual', 'Indie']</td>\n",
       "      <td>['Action', 'Indie', 'Casual', 'Violent', 'Adve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120440</th>\n",
       "      <td>['Casual', 'Indie', 'Simulation', 'Strategy']</td>\n",
       "      <td>['Strategy', 'Indie', 'Casual', 'Simulation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120441</th>\n",
       "      <td>['Casual', 'Indie', 'Strategy']</td>\n",
       "      <td>['Strategy', 'Indie', 'Casual']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120442</th>\n",
       "      <td>['Indie', 'Racing', 'Simulation']</td>\n",
       "      <td>['Indie', 'Simulation', 'Racing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120443</th>\n",
       "      <td>['Casual', 'Indie']</td>\n",
       "      <td>['Indie', 'Casual', 'Puzzle', 'Singleplayer', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   genres  \\\n",
       "88310   ['Action', 'Casual', 'Indie', 'Simulation', 'S...   \n",
       "88311        ['Free to Play', 'Indie', 'RPG', 'Strategy']   \n",
       "88312   ['Casual', 'Free to Play', 'Indie', 'Simulatio...   \n",
       "88313                   ['Action', 'Adventure', 'Casual']   \n",
       "88315               ['Action', 'Adventure', 'Simulation']   \n",
       "...                                                   ...   \n",
       "120439         ['Action', 'Adventure', 'Casual', 'Indie']   \n",
       "120440      ['Casual', 'Indie', 'Simulation', 'Strategy']   \n",
       "120441                    ['Casual', 'Indie', 'Strategy']   \n",
       "120442                  ['Indie', 'Racing', 'Simulation']   \n",
       "120443                                ['Casual', 'Indie']   \n",
       "\n",
       "                                                     tags  \n",
       "88310   ['Strategy', 'Action', 'Indie', 'Casual', 'Sim...  \n",
       "88311   ['Free to Play', 'Strategy', 'Indie', 'RPG', '...  \n",
       "88312   ['Free to Play', 'Simulation', 'Sports', 'Casu...  \n",
       "88313                   ['Action', 'Adventure', 'Casual']  \n",
       "88315   ['Action', 'Adventure', 'Simulation', 'FPS', '...  \n",
       "...                                                   ...  \n",
       "120439  ['Action', 'Indie', 'Casual', 'Violent', 'Adve...  \n",
       "120440      ['Strategy', 'Indie', 'Casual', 'Simulation']  \n",
       "120441                    ['Strategy', 'Indie', 'Casual']  \n",
       "120442                  ['Indie', 'Simulation', 'Racing']  \n",
       "120443  ['Indie', 'Casual', 'Puzzle', 'Singleplayer', ...  \n",
       "\n",
       "[28821 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[['genres', 'tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\3010365759.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  pdf['genres'].fillna('', inplace=True)\n",
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\3010365759.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  pdf['tags'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar los valores nulos en la columna 'genres' con una cadena vacía\n",
    "pdf['genres'].fillna('', inplace=True)\n",
    "\n",
    "# Reemplazar los valores nulos en la columna 'tags' con una cadena vacía\n",
    "pdf['tags'].fillna('', inplace=True)\n",
    "\n",
    "# Función para combinar los valores de 'genres' y 'tags'\n",
    "def combine_genres_tags(row):\n",
    "    genres = row['genres']\n",
    "    tags = row['tags']\n",
    "    \n",
    "    # Si 'genres' está vacío, asigna los valores de 'tags'\n",
    "    if not genres:\n",
    "        return tags\n",
    "    \n",
    "    # Si 'tags' tiene más información que 'genres', combina ambas\n",
    "    if tags:\n",
    "        tags_list = tags.split(', ')\n",
    "        for tag in tags_list:\n",
    "            if tag not in genres:\n",
    "                genres += ', ' + tag\n",
    "    return genres\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame\n",
    "pdf['genres'] = pdf.apply(combine_genres_tags, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, ya podemos solucionar el primer ENDPOINT solicitado, para el cual necesitábamos ingresar un string con un Developer, y nos arrojará el porcentaje de juegos gratuitos que salieron cada año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = pa.Table.from_pandas(pdf)\n",
    "# Escribimos las tabla en un archivo Parquet\n",
    "pq.write_table(table, 'Datasets/pdf_SteamGames.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos con el segundo DATAFRAME, en este caso, Steam Games. El mismo, tiene errores de escritura por lo cual debemos cambiar el proceso de carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>dmckay</td>\n",
       "      <td>http://steamcommunity.com/id/dmckay</td>\n",
       "      <td>[{'funny': '', 'posted': 'Posted January 2.', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22074</th>\n",
       "      <td>76561198069625198</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561198069...</td>\n",
       "      <td>[{'funny': '', 'posted': 'Posted October 27, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id                                           user_url  \\\n",
       "7567              dmckay                http://steamcommunity.com/id/dmckay   \n",
       "22074  76561198069625198  http://steamcommunity.com/profiles/76561198069...   \n",
       "\n",
       "                                                 reviews  \n",
       "7567   [{'funny': '', 'posted': 'Posted January 2.', ...  \n",
       "22074  [{'funny': '', 'posted': 'Posted October 27, 2...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "#Creamos una lista vacía llamada \"rows\" donde almacenaremos los datos del archivo JSON.\n",
    "rows = []\n",
    "#Abrir el archivo \"user_reviews.json/australian_user_reviews.json\" con la codificación MacRoman.\n",
    "with open(r\"Datasets\\user_reviews.json\", encoding='utf-8') as f:\n",
    "    # Leer cada línea del archivo.\n",
    "    for line in f.readlines():\n",
    "        # Utilizar \"ast.literal_eval\" para convertir cada línea en un diccionario de Python\n",
    "        # y agregarlo a la lista \"rows\".\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "#Crear un DataFrame de Pandas a partir de la lista de diccionarios \"rows\".\n",
    "df_user_reviews = pd.DataFrame(rows)\n",
    "#Veamos unos registros al asar\n",
    "df_user_reviews.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos función para analizar el estado del Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general del DataFrame:\n",
      "Cantidad de filas: 25799\n",
      "Cantidad de columnas: 3\n",
      "Cantidad de filas con datos nulos: 0\n",
      "\n",
      "Porcentaje de datos nulos por fila:\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "25794    0.0\n",
      "25795    0.0\n",
      "25796    0.0\n",
      "25797    0.0\n",
      "25798    0.0\n",
      "Length: 25799, dtype: float64\n",
      "\n",
      "Porcentaje de datos nulos por columna:\n",
      "user_id     0.0\n",
      "user_url    0.0\n",
      "reviews     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "infoDF(df_user_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataframe no contiene datos nulos pero nos encontramos con un nuevo desafío. La fila de reviews es una cantidad de listas anidadas que debemos desanidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna reviews:\n",
      "[{'funny': '', 'posted': 'Posted November 5, 2011.', 'last_edited': '', 'item_id': '1250', 'helpful': 'No ratings yet', 'recommend': True, 'review': 'Simple yet with great replayability. In my opinion does \"zombie\" hordes and team work better than left 4 dead plus has a global leveling system. Alot of down to earth \"zombie\" splattering fun for the whole family. Amazed this sort of FPS is so rare.'}, {'funny': '', 'posted': 'Posted July 15, 2011.', 'last_edited': '', 'item_id': '22200', 'helpful': 'No ratings yet', 'recommend': True, 'review': \"It's unique and worth a playthrough.\"}, {'funny': '', 'posted': 'Posted April 21, 2011.', 'last_edited': '', 'item_id': '43110', 'helpful': 'No ratings yet', 'recommend': True, 'review': 'Great atmosphere. The gunplay can be a bit chunky at times but at the end of the day this game is definitely worth it and I hope they do a sequel...so buy the game so I get a sequel!'}]\n",
      "[<class 'list'>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna reviews:\")\n",
    "print(df_user_reviews['reviews'].iloc[0])  # Imprimir un valor específico de la columna 'reviews'\n",
    "print(df_user_reviews['reviews'].apply(type).unique())  # Verificar el tipo de dato de toda la columna 'reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que nuestro Dataframe tenga sentido, debemos desarmar esta lista y crear columnas por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 user_id                                           user_url  \\\n",
      "0      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "1      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "2      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "3                js41637               http://steamcommunity.com/id/js41637   \n",
      "4                js41637               http://steamcommunity.com/id/js41637   \n",
      "...                  ...                                                ...   \n",
      "59300  76561198312638244  http://steamcommunity.com/profiles/76561198312...   \n",
      "59301  76561198312638244  http://steamcommunity.com/profiles/76561198312...   \n",
      "59302        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "59303        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "59304        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "\n",
      "                                  funny                     posted  \\\n",
      "0                                         Posted November 5, 2011.   \n",
      "1                                            Posted July 15, 2011.   \n",
      "2                                           Posted April 21, 2011.   \n",
      "3                                            Posted June 24, 2014.   \n",
      "4                                        Posted September 8, 2013.   \n",
      "...                                 ...                        ...   \n",
      "59300                                              Posted July 10.   \n",
      "59301                                               Posted July 8.   \n",
      "59302  1 person found this review funny             Posted July 3.   \n",
      "59303                                              Posted July 20.   \n",
      "59304                                               Posted July 2.   \n",
      "\n",
      "      last_edited item_id                                          helpful  \\\n",
      "0                    1250                                   No ratings yet   \n",
      "1                   22200                                   No ratings yet   \n",
      "2                   43110                                   No ratings yet   \n",
      "3                  251610  15 of 20 people (75%) found this review helpful   \n",
      "4                  227300     0 of 1 people (0%) found this review helpful   \n",
      "...           ...     ...                                              ...   \n",
      "59300                  70                                   No ratings yet   \n",
      "59301              362890                                   No ratings yet   \n",
      "59302              273110    1 of 2 people (50%) found this review helpful   \n",
      "59303                 730                                   No ratings yet   \n",
      "59304                 440                                   No ratings yet   \n",
      "\n",
      "       recommend                                             review  \n",
      "0           True  Simple yet with great replayability. In my opi...  \n",
      "1           True               It's unique and worth a playthrough.  \n",
      "2           True  Great atmosphere. The gunplay can be a bit chu...  \n",
      "3           True  I know what you think when you see this title ...  \n",
      "4           True  For a simple (it's actually not all that simpl...  \n",
      "...          ...                                                ...  \n",
      "59300       True  a must have classic from steam definitely wort...  \n",
      "59301       True  this game is a perfect remake of the original ...  \n",
      "59302       True  had so much fun plaing this and collecting res...  \n",
      "59303       True                                                 :D  \n",
      "59304       True                                     so much fun :D  \n",
      "\n",
      "[59305 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear una lista vacía para almacenar las nuevas filas\n",
    "new_rows = []\n",
    "\n",
    "# Iterar sobre cada fila del DataFrame original\n",
    "for index, row in df_user_reviews.iterrows():\n",
    "    # Obtener la lista de reviews de la fila actual\n",
    "    reviews_list = row.get('reviews')\n",
    "    # Verificar si la lista de reviews está presente y no es None\n",
    "    if reviews_list is not None:\n",
    "        # Iterar sobre cada review en la lista\n",
    "        for review_dict in reviews_list:\n",
    "            # Crear una nueva fila con los datos de la review y otros datos de la fila original\n",
    "            new_row = {\n",
    "                'user_id': row['user_id'],\n",
    "                'user_url': row['user_url'],\n",
    "                'funny': review_dict.get('funny'),\n",
    "                'posted': review_dict.get('posted'),\n",
    "                'last_edited': review_dict.get('last_edited'),\n",
    "                'item_id': review_dict.get('item_id'),\n",
    "                'helpful': review_dict.get('helpful'),\n",
    "                'recommend': review_dict.get('recommend'),\n",
    "                'review': review_dict.get('review')\n",
    "            }\n",
    "            # Agregar la nueva fila a la lista de filas\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con las nuevas filas\n",
    "new_df_user_reviews = pd.DataFrame(new_rows)\n",
    "\n",
    "# Ver el nuevo DataFrame\n",
    "print(new_df_user_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho mejor, ahora corroboramos que no existan filas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 874 filas duplicadas en el DataFrame.\n"
     ]
    }
   ],
   "source": [
    "filas_duplicadas = new_df_user_reviews[new_df_user_reviews.duplicated()]\n",
    "num_filas_duplicadas = len(filas_duplicadas)\n",
    "if num_filas_duplicadas > 0:\n",
    "    print(\"Se encontraron\", num_filas_duplicadas, \"filas duplicadas en el DataFrame.\")\n",
    "else:\n",
    "    print(\"No se encontraron filas duplicadas en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se encontraron muchas, asi que toca eliminarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas duplicadas en todo el DataFrame\n",
    "new_df_user_reviews_sin_duplicados = new_df_user_reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo listo, ahora eliminamos las columnas que no tienen valor para nuestro futuro análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['funny', 'last_edited', 'helpful'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Eliminar columnas usando el método drop()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m columnas_a_eliminar \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunny\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_edited\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpful\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnew_df_user_reviews_sin_duplicados\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumnas_a_eliminar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Verificar que las columnas se han eliminado\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_df_user_reviews_sin_duplicados\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['funny', 'last_edited', 'helpful'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Eliminar columnas usando el método drop()\n",
    "columnas_a_eliminar = ['funny', 'last_edited', 'helpful']\n",
    "new_df_user_reviews_sin_duplicados.drop(columnas_a_eliminar, axis=1, inplace=True)\n",
    "\n",
    "# Verificar que las columnas se han eliminado\n",
    "print(new_df_user_reviews_sin_duplicados.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y empezamos la corrección de tipos de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id      object\n",
      "user_url     object\n",
      "posted       object\n",
      "item_id      object\n",
      "recommend      bool\n",
      "review       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_df_user_reviews_sin_duplicados.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos corregir es la columna 'posted', ya que la misma no coincide dentro de los datos y nos complicaría el funcionamiento de la función. Para ello crearemos una nueva columna 'year' donde extraeremos únicamente el año en que fue publicada la reseña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 user_id                                           user_url  \\\n",
      "0      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "1      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "2      76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "3                js41637               http://steamcommunity.com/id/js41637   \n",
      "4                js41637               http://steamcommunity.com/id/js41637   \n",
      "...                  ...                                                ...   \n",
      "59300  76561198312638244  http://steamcommunity.com/profiles/76561198312...   \n",
      "59301  76561198312638244  http://steamcommunity.com/profiles/76561198312...   \n",
      "59302        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "59303        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "59304        LydiaMorley           http://steamcommunity.com/id/LydiaMorley   \n",
      "\n",
      "                          posted item_id  recommend  \\\n",
      "0       Posted November 5, 2011.    1250       True   \n",
      "1          Posted July 15, 2011.   22200       True   \n",
      "2         Posted April 21, 2011.   43110       True   \n",
      "3          Posted June 24, 2014.  251610       True   \n",
      "4      Posted September 8, 2013.  227300       True   \n",
      "...                          ...     ...        ...   \n",
      "59300            Posted July 10.      70       True   \n",
      "59301             Posted July 8.  362890       True   \n",
      "59302             Posted July 3.  273110       True   \n",
      "59303            Posted July 20.     730       True   \n",
      "59304             Posted July 2.     440       True   \n",
      "\n",
      "                                                  review  year  \n",
      "0      Simple yet with great replayability. In my opi...  2011  \n",
      "1                   It's unique and worth a playthrough.  2011  \n",
      "2      Great atmosphere. The gunplay can be a bit chu...  2011  \n",
      "3      I know what you think when you see this title ...  2014  \n",
      "4      For a simple (it's actually not all that simpl...  2013  \n",
      "...                                                  ...   ...  \n",
      "59300  a must have classic from steam definitely wort...  2014  \n",
      "59301  this game is a perfect remake of the original ...  2014  \n",
      "59302  had so much fun plaing this and collecting res...  2014  \n",
      "59303                                                 :D  2014  \n",
      "59304                                     so much fun :D  2014  \n",
      "\n",
      "[58431 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\846802356.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_user_reviews_sin_duplicados['year'] = new_df_user_reviews_sin_duplicados['posted'].apply(extract_year)\n",
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\846802356.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_df_user_reviews_sin_duplicados['year'].fillna(average_year, inplace=True)\n",
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\846802356.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_user_reviews_sin_duplicados['year'].fillna(average_year, inplace=True)\n",
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\846802356.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_user_reviews_sin_duplicados['year'] = new_df_user_reviews_sin_duplicados['year'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Función para extraer el año de la columna 'posted'\n",
    "def extract_year(date_string):\n",
    "    # Expresión regular para encontrar el año\n",
    "    year_pattern = r'\\b\\d{4}\\b'\n",
    "    # Buscar el año en el texto\n",
    "    year_match = re.search(year_pattern, date_string)\n",
    "    if year_match:\n",
    "        return int(year_match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Aplicar la función para extraer el año y crear una nueva columna 'year'\n",
    "new_df_user_reviews_sin_duplicados['year'] = new_df_user_reviews_sin_duplicados['posted'].apply(extract_year)\n",
    "\n",
    "# Calcular el promedio de los años presentes en el dataframe\n",
    "average_year = new_df_user_reviews_sin_duplicados['year'].mean()\n",
    "\n",
    "# Rellenar los valores de año faltantes con el promedio\n",
    "new_df_user_reviews_sin_duplicados['year'].fillna(average_year, inplace=True)\n",
    "\n",
    "# Convertir los años a enteros\n",
    "new_df_user_reviews_sin_duplicados['year'] = new_df_user_reviews_sin_duplicados['year'].astype(int)\n",
    "\n",
    "# Imprimir el dataframe resultante\n",
    "print(new_df_user_reviews_sin_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\3375310032.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_user_reviews_sin_duplicados.drop(columns=['posted'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "new_df_user_reviews_sin_duplicados.dtypes\n",
    "new_df_user_reviews_sin_duplicados.drop(columns=['posted'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\342199010.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_user_reviews_sin_duplicados['item_id'] = new_df_user_reviews_sin_duplicados['item_id'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'item_id' a tipo float\n",
    "new_df_user_reviews_sin_duplicados['item_id'] = new_df_user_reviews_sin_duplicados['item_id'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id       object\n",
       "user_url      object\n",
       "item_id      float64\n",
       "recommend       bool\n",
       "review        object\n",
       "year           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_user_reviews_sin_duplicados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(new_df_user_reviews_sin_duplicados)\n",
    "# Escribimos las tabla en un archivo Parquet\n",
    "pq.write_table(table, 'Datasets/new_user_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos la nueva tabla en parquet para su futuro uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos el último ETL, del json user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# Lista para almacenar los datos del archivo JSON\n",
    "rows = []\n",
    "\n",
    "# Abrir el archivo comprimido en modo lectura de bytes ('rb')\n",
    "with gzip.open(r\"Datasets\\users_items.json.gz\", 'rb') as f:\n",
    "    # Decodificar cada línea del archivo como texto UTF-8 y agregarla a la lista 'rows'\n",
    "    for line in f:\n",
    "        rows.append(line.decode('utf-8'))\n",
    "\n",
    "# Crear un DataFrame de Pandas a partir de la lista de strings\n",
    "df_user_items = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corroboramos la información de nuestro nuevo Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general del DataFrame:\n",
      "Cantidad de filas: 88310\n",
      "Cantidad de columnas: 1\n",
      "Cantidad de filas con datos nulos: 0\n",
      "\n",
      "Porcentaje de datos nulos por fila:\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "88305    0.0\n",
      "88306    0.0\n",
      "88307    0.0\n",
      "88308    0.0\n",
      "88309    0.0\n",
      "Length: 88310, dtype: float64\n",
      "\n",
      "Porcentaje de datos nulos por columna:\n",
      "0    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_user_items.head()\n",
    "infoDF(df_user_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, aquí el error se encuentra en que existe únicamente una sola columna con toda la información anidada dentro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df_user_items.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos el proceso de desanidar todo nuestro dataframe para poder utilizarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   user_id  items_count           steam_id  \\\n",
      "0        76561197970982479          277  76561197970982479   \n",
      "1        76561197970982479          277  76561197970982479   \n",
      "2        76561197970982479          277  76561197970982479   \n",
      "3        76561197970982479          277  76561197970982479   \n",
      "4        76561197970982479          277  76561197970982479   \n",
      "...                    ...          ...                ...   \n",
      "5153204  76561198329548331            7  76561198329548331   \n",
      "5153205  76561198329548331            7  76561198329548331   \n",
      "5153206  76561198329548331            7  76561198329548331   \n",
      "5153207  76561198329548331            7  76561198329548331   \n",
      "5153208  76561198329548331            7  76561198329548331   \n",
      "\n",
      "                                                  user_url item_id  \\\n",
      "0        http://steamcommunity.com/profiles/76561197970...      10   \n",
      "1        http://steamcommunity.com/profiles/76561197970...      20   \n",
      "2        http://steamcommunity.com/profiles/76561197970...      30   \n",
      "3        http://steamcommunity.com/profiles/76561197970...      40   \n",
      "4        http://steamcommunity.com/profiles/76561197970...      50   \n",
      "...                                                    ...     ...   \n",
      "5153204  http://steamcommunity.com/profiles/76561198329...  346330   \n",
      "5153205  http://steamcommunity.com/profiles/76561198329...  373330   \n",
      "5153206  http://steamcommunity.com/profiles/76561198329...  388490   \n",
      "5153207  http://steamcommunity.com/profiles/76561198329...  521570   \n",
      "5153208  http://steamcommunity.com/profiles/76561198329...  519140   \n",
      "\n",
      "                             item_name  playtime_forever  playtime_2weeks  \n",
      "0                       Counter-Strike                 6                0  \n",
      "1                Team Fortress Classic                 0                0  \n",
      "2                        Day of Defeat                 7                0  \n",
      "3                   Deathmatch Classic                 0                0  \n",
      "4            Half-Life: Opposing Force                 0                0  \n",
      "...                                ...               ...              ...  \n",
      "5153204                   BrainBread 2                 0                0  \n",
      "5153205                    All Is Dust                 0                0  \n",
      "5153206  One Way To Die: Steam Edition                 3                3  \n",
      "5153207          You Have 10 Seconds 2                 4                4  \n",
      "5153208                     Minds Eyes                 3                3  \n",
      "\n",
      "[5153209 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear una función para procesar cada fila del DataFrame original\n",
    "def process_row(row):\n",
    "    # Convertir el string de la columna 0 a un diccionario\n",
    "    df_user_items = ast.literal_eval(row.iloc[0])\n",
    "    \n",
    "    # Extraer información\n",
    "    user_id = df_user_items['user_id']\n",
    "    items_count = df_user_items['items_count']\n",
    "    steam_id = df_user_items['steam_id']\n",
    "    user_url = df_user_items['user_url']\n",
    "    \n",
    "    # Crear una lista de diccionarios para los 'items'\n",
    "    items_list = df_user_items['items']\n",
    "    \n",
    "    # Crear una lista para almacenar cada fila de datos\n",
    "    rows = []\n",
    "    \n",
    "    # Iterar sobre los elementos de la lista de items\n",
    "    for item in items_list:\n",
    "        # Crear un diccionario con la información que necesitas\n",
    "        row_df_user_items = {\n",
    "            'user_id': user_id,\n",
    "            'items_count': items_count,\n",
    "            'steam_id': steam_id,\n",
    "            'user_url': user_url,\n",
    "            'item_id': item.get('item_id', None),  # Manejar casos donde el valor no está presente\n",
    "            'item_name': item.get('item_name', None),\n",
    "            'playtime_forever': item.get('playtime_forever', None),\n",
    "            'playtime_2weeks': item.get('playtime_2weeks', None)\n",
    "        }\n",
    "        # Agregar el diccionario a la lista de filas\n",
    "        rows.append(row_df_user_items)\n",
    "    \n",
    "    # Retornar la lista de filas\n",
    "    return rows\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame original y obtener una lista de listas de diccionarios\n",
    "processed_data = df_user_items.apply(process_row, axis=1).tolist()\n",
    "\n",
    "# Convertir la lista de listas de diccionarios en un solo DataFrame\n",
    "new_df = pd.DataFrame([item for sublist in processed_data for item in sublist])\n",
    "\n",
    "# Mostrar el nuevo DataFrame\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos empezar a trabajar sobre nuestro dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 59104 filas duplicadas en el DataFrame.\n"
     ]
    }
   ],
   "source": [
    "filas_duplicadas = new_df[new_df.duplicated()]\n",
    "num_filas_duplicadas = len(filas_duplicadas)\n",
    "if num_filas_duplicadas > 0:\n",
    "    print(\"Se encontraron\", num_filas_duplicadas, \"filas duplicadas en el DataFrame.\")\n",
    "else:\n",
    "    print(\"No se encontraron filas duplicadas en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los duplicados y vemos el tipo de datos que contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_sin_duplicados = new_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id             object\n",
       "items_count          int64\n",
       "steam_id            object\n",
       "user_url            object\n",
       "item_id             object\n",
       "item_name           object\n",
       "playtime_forever     int64\n",
       "playtime_2weeks      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_sin_duplicados.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar nuestros dataframes en conjunto, necesitamos que item_id sea tipo float y que item_name sea tipo string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\3657668227.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_sin_duplicados['item_id'] = new_df_sin_duplicados['item_id'].astype(float)\n",
      "C:\\Users\\loren\\AppData\\Local\\Temp\\ipykernel_18864\\3657668227.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_sin_duplicados['item_name'] = new_df_sin_duplicados['item_name'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'item_id' a tipo float\n",
    "new_df_sin_duplicados['item_id'] = new_df_sin_duplicados['item_id'].astype(float)\n",
    "\n",
    "# Convertir la columna 'item_name' a tipo string\n",
    "new_df_sin_duplicados['item_name'] = new_df_sin_duplicados['item_name'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo listo, podemos guardar nuestros dataframes en Parquet, y continuar el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(new_df_sin_duplicados)\n",
    "# Escribimos las tabla en un archivo Parquet\n",
    "pq.write_table(table, 'Datasets/new_df_users_items.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
